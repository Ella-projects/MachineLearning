{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Atomix77/IMLO-IP/blob/main/IMLO_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rPqbMcduQaTJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://thor.robots.ox.ac.uk/flowers/102/102flowers.tgz to datasets/flowers-102/102flowers.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 344862509/344862509 [00:11<00:00, 29678530.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/flowers-102/102flowers.tgz to datasets/flowers-102\n",
      "Downloading https://thor.robots.ox.ac.uk/flowers/102/imagelabels.mat to datasets/flowers-102/imagelabels.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 502/502 [00:00<00:00, 382061.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://thor.robots.ox.ac.uk/flowers/102/setid.mat to datasets/flowers-102/setid.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 14989/14989 [00:00<00:00, 4561632.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "trainingTransform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.RandomRotation(180),\n",
    "    transforms.RandomAutocontrast(),\n",
    "    transforms.RandomAdjustSharpness(1.5, 0.5),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "    ])\n",
    "\n",
    "validationTransform = transforms.Compose([ \n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "    ])\n",
    "\n",
    "testTransform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "    ])\n",
    "\n",
    "trainingData = datasets.Flowers102(\n",
    "    root = \"./datasets\",\n",
    "    split = \"train\",\n",
    "    transform = trainingTransform,\n",
    "    download = True)\n",
    "\n",
    "validationData = datasets.Flowers102(\n",
    "    root = \"./datasets\",\n",
    "    split = \"val\",\n",
    "    transform = validationTransform,\n",
    "    download = True)\n",
    "\n",
    "testData = datasets.Flowers102(\n",
    "    root = \"./datasets\",\n",
    "    split = \"test\",\n",
    "    transform = testTransform,\n",
    "    download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lJxe_JHFdp5v"
   },
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "trainingDataloader = DataLoader(trainingData, batch_size = 8, shuffle = True, num_workers = 6)\n",
    "validationDataloader = DataLoader(validationData, batch_size = 32, shuffle = False, num_workers = 6)\n",
    "testDataloader = DataLoader(testData, batch_size = 32, shuffle = False, num_workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOZbLdGEsfV2",
    "outputId": "f2f6f56c-12bb-47eb-e8bc-3e8c298ef4bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DEtkf2DSxjNd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "# Create Aritecture\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, classAmount):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.convStack =  nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "\n",
    "            nn.Conv2d(32, 32, 3, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),          \n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(128, 128, 3, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(256, 256, 3, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        dummyInput = torch.zeros(1, 3, 256, 256)\n",
    "        dummyOutput = self.convStack(dummyInput)\n",
    "        self.convOutputSize = dummyOutput.view(1, -1).size(1)\n",
    "        print(self.convOutputSize)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.convOutputSize, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, classAmount)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convStack(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork(102).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "s1wDws8xJOfQ"
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "losses = []\n",
    "accuracy = []\n",
    "def trainModel(dataloader, model, lossFunction, optimizer):\n",
    "    model.train()\n",
    "    currentLoss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    epochLoss = 0\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        total += y.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = lossFunction(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        currentLoss += loss.item()\n",
    "        epochLoss += lossFunction(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    epochLoss = epochLoss/total\n",
    "    correct = correct/total\n",
    "\n",
    "    losses.append(epochLoss)\n",
    "    accuracy.append(correct * 100)\n",
    "    print(f'Training: Accuracy {correct * 100:>0.1f}%, Loss: {currentLoss / len(dataloader):.5f}, Epoch Loss: {epochLoss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Model\n",
    "def validateModel(dataloader, model, lossFunction):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epochLoss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            total += y.size(0)\n",
    "            epochLoss += lossFunction(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    epochLoss = epochLoss/total\n",
    "    correct = (correct/total) * 100\n",
    "\n",
    "    print(f\"Validation: Accuracy: {(correct):>0.1f}%, Avg loss: {epochLoss:>8f} \\n\")\n",
    "    return epochLoss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Model\n",
    "def testModel(dataloader, model, lossFunction):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    testLoss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            total += y.size(0)\n",
    "            testLoss += lossFunction(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    testLoss = testLoss/total\n",
    "    correct = (correct/total) * 100\n",
    "    print(f\"Testing: Accuracy: {(correct):>0.1f}%, Avg loss: {testLoss:>8f} \\n\")\n",
    "    return testLoss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "-8n5NAvIJUuO",
    "outputId": "50d3d6ee-ddaa-4bba-f3e4-d5f5448cb12b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Training: Accuracy 0.7%, Loss: 4.82547, Epoch Loss: 0.60555\n",
      "Validation: Accuracy: 0.6%, Avg loss: 0.144152 \n",
      "\n",
      "Epoch 2:\n",
      "Training: Accuracy 1.0%, Loss: 4.72876, Epoch Loss: 0.59341\n",
      "Validation: Accuracy: 2.4%, Avg loss: 0.141683 \n",
      "\n",
      "Epoch 3:\n",
      "Training: Accuracy 0.6%, Loss: 4.68575, Epoch Loss: 0.58802\n",
      "Validation: Accuracy: 3.2%, Avg loss: 0.138922 \n",
      "\n",
      "Epoch 4:\n",
      "Training: Accuracy 2.6%, Loss: 4.58092, Epoch Loss: 0.57486\n",
      "Validation: Accuracy: 2.9%, Avg loss: 0.136312 \n",
      "\n",
      "Epoch 5:\n",
      "Training: Accuracy 2.8%, Loss: 4.50350, Epoch Loss: 0.56515\n",
      "Validation: Accuracy: 4.0%, Avg loss: 0.135507 \n",
      "\n",
      "Epoch 6:\n",
      "Training: Accuracy 2.1%, Loss: 4.44768, Epoch Loss: 0.55814\n",
      "Validation: Accuracy: 3.8%, Avg loss: 0.130524 \n",
      "\n",
      "Epoch 7:\n",
      "Training: Accuracy 2.5%, Loss: 4.36360, Epoch Loss: 0.54759\n",
      "Validation: Accuracy: 4.9%, Avg loss: 0.129982 \n",
      "\n",
      "Epoch 8:\n",
      "Training: Accuracy 3.1%, Loss: 4.32853, Epoch Loss: 0.54319\n",
      "Validation: Accuracy: 5.1%, Avg loss: 0.125417 \n",
      "\n",
      "Epoch 9:\n",
      "Training: Accuracy 3.6%, Loss: 4.27639, Epoch Loss: 0.53665\n",
      "Validation: Accuracy: 5.2%, Avg loss: 0.125585 \n",
      "\n",
      "Epoch 10:\n",
      "Training: Accuracy 4.0%, Loss: 4.24922, Epoch Loss: 0.53323\n",
      "Validation: Accuracy: 7.2%, Avg loss: 0.122284 \n",
      "\n",
      "Epoch 11:\n",
      "Training: Accuracy 4.3%, Loss: 4.18304, Epoch Loss: 0.52493\n",
      "Validation: Accuracy: 6.6%, Avg loss: 0.122299 \n",
      "\n",
      "Epoch 12:\n",
      "Training: Accuracy 4.7%, Loss: 4.17133, Epoch Loss: 0.52346\n",
      "Validation: Accuracy: 7.1%, Avg loss: 0.122847 \n",
      "\n",
      "Epoch 13:\n",
      "Training: Accuracy 4.1%, Loss: 4.16311, Epoch Loss: 0.52243\n",
      "Validation: Accuracy: 8.5%, Avg loss: 0.121547 \n",
      "\n",
      "Epoch 14:\n",
      "Training: Accuracy 4.7%, Loss: 4.12929, Epoch Loss: 0.51819\n",
      "Validation: Accuracy: 7.5%, Avg loss: 0.121043 \n",
      "\n",
      "Epoch 15:\n",
      "Training: Accuracy 5.2%, Loss: 4.13830, Epoch Loss: 0.51932\n",
      "Validation: Accuracy: 10.0%, Avg loss: 0.117087 \n",
      "\n",
      "Epoch 16:\n",
      "Training: Accuracy 6.2%, Loss: 4.03564, Epoch Loss: 0.50643\n",
      "Validation: Accuracy: 9.0%, Avg loss: 0.117892 \n",
      "\n",
      "Epoch 17:\n",
      "Training: Accuracy 4.5%, Loss: 4.03382, Epoch Loss: 0.50620\n",
      "Validation: Accuracy: 9.5%, Avg loss: 0.119692 \n",
      "\n",
      "Epoch 18:\n",
      "Training: Accuracy 5.4%, Loss: 4.03557, Epoch Loss: 0.50642\n",
      "Validation: Accuracy: 8.7%, Avg loss: 0.116107 \n",
      "\n",
      "Epoch 19:\n",
      "Training: Accuracy 6.8%, Loss: 4.02626, Epoch Loss: 0.50526\n",
      "Validation: Accuracy: 8.4%, Avg loss: 0.115175 \n",
      "\n",
      "Epoch 20:\n",
      "Training: Accuracy 3.9%, Loss: 4.04242, Epoch Loss: 0.50728\n",
      "Validation: Accuracy: 10.8%, Avg loss: 0.113833 \n",
      "\n",
      "Epoch 21:\n",
      "Training: Accuracy 5.0%, Loss: 4.01616, Epoch Loss: 0.50399\n",
      "Validation: Accuracy: 7.9%, Avg loss: 0.117357 \n",
      "\n",
      "Epoch 22:\n",
      "Training: Accuracy 5.5%, Loss: 3.99387, Epoch Loss: 0.50119\n",
      "Validation: Accuracy: 9.0%, Avg loss: 0.116042 \n",
      "\n",
      "Epoch 23:\n",
      "Training: Accuracy 5.6%, Loss: 4.00489, Epoch Loss: 0.50257\n",
      "Validation: Accuracy: 11.3%, Avg loss: 0.112471 \n",
      "\n",
      "Epoch 24:\n",
      "Training: Accuracy 6.6%, Loss: 3.96283, Epoch Loss: 0.49730\n",
      "Validation: Accuracy: 9.3%, Avg loss: 0.111894 \n",
      "\n",
      "Epoch 25:\n",
      "Training: Accuracy 6.1%, Loss: 3.92138, Epoch Loss: 0.49209\n",
      "Validation: Accuracy: 9.8%, Avg loss: 0.112327 \n",
      "\n",
      "Epoch 26:\n",
      "Training: Accuracy 4.4%, Loss: 3.96853, Epoch Loss: 0.49801\n",
      "Validation: Accuracy: 10.2%, Avg loss: 0.109229 \n",
      "\n",
      "Epoch 27:\n",
      "Training: Accuracy 5.5%, Loss: 3.92106, Epoch Loss: 0.49205\n",
      "Validation: Accuracy: 10.9%, Avg loss: 0.110754 \n",
      "\n",
      "Epoch 28:\n",
      "Training: Accuracy 6.1%, Loss: 3.90224, Epoch Loss: 0.48969\n",
      "Validation: Accuracy: 11.0%, Avg loss: 0.110654 \n",
      "\n",
      "Epoch 29:\n",
      "Training: Accuracy 6.6%, Loss: 3.84237, Epoch Loss: 0.48218\n",
      "Validation: Accuracy: 8.3%, Avg loss: 0.113450 \n",
      "\n",
      "Epoch 30:\n",
      "Training: Accuracy 5.6%, Loss: 3.87128, Epoch Loss: 0.48581\n",
      "Validation: Accuracy: 11.1%, Avg loss: 0.108139 \n",
      "\n",
      "Epoch 31:\n",
      "Training: Accuracy 6.1%, Loss: 3.88938, Epoch Loss: 0.48808\n",
      "Validation: Accuracy: 12.2%, Avg loss: 0.108168 \n",
      "\n",
      "Epoch 32:\n",
      "Training: Accuracy 5.9%, Loss: 3.88228, Epoch Loss: 0.48719\n",
      "Validation: Accuracy: 9.9%, Avg loss: 0.109537 \n",
      "\n",
      "Epoch 33:\n",
      "Training: Accuracy 6.0%, Loss: 3.83975, Epoch Loss: 0.48185\n",
      "Validation: Accuracy: 10.0%, Avg loss: 0.113065 \n",
      "\n",
      "Epoch 34:\n",
      "Training: Accuracy 5.7%, Loss: 3.86820, Epoch Loss: 0.48542\n",
      "Validation: Accuracy: 10.8%, Avg loss: 0.111237 \n",
      "\n",
      "Epoch 35:\n",
      "Training: Accuracy 6.7%, Loss: 3.85044, Epoch Loss: 0.48319\n",
      "Validation: Accuracy: 12.2%, Avg loss: 0.107894 \n",
      "\n",
      "Epoch 36:\n",
      "Training: Accuracy 6.5%, Loss: 3.81872, Epoch Loss: 0.47921\n",
      "Validation: Accuracy: 11.1%, Avg loss: 0.108618 \n",
      "\n",
      "Epoch 37:\n",
      "Training: Accuracy 5.5%, Loss: 3.84111, Epoch Loss: 0.48202\n",
      "Validation: Accuracy: 12.4%, Avg loss: 0.106991 \n",
      "\n",
      "Epoch 38:\n",
      "Training: Accuracy 6.6%, Loss: 3.81195, Epoch Loss: 0.47836\n",
      "Validation: Accuracy: 12.9%, Avg loss: 0.107701 \n",
      "\n",
      "Epoch 39:\n",
      "Training: Accuracy 7.4%, Loss: 3.78455, Epoch Loss: 0.47492\n",
      "Validation: Accuracy: 11.0%, Avg loss: 0.109647 \n",
      "\n",
      "Epoch 40:\n",
      "Training: Accuracy 7.6%, Loss: 3.74283, Epoch Loss: 0.46969\n",
      "Validation: Accuracy: 14.3%, Avg loss: 0.108054 \n",
      "\n",
      "Epoch 41:\n",
      "Training: Accuracy 6.8%, Loss: 3.75654, Epoch Loss: 0.47141\n",
      "Validation: Accuracy: 14.5%, Avg loss: 0.106228 \n",
      "\n",
      "Epoch 42:\n",
      "Training: Accuracy 6.4%, Loss: 3.78939, Epoch Loss: 0.47553\n",
      "Validation: Accuracy: 13.7%, Avg loss: 0.106692 \n",
      "\n",
      "Epoch 43:\n",
      "Training: Accuracy 8.8%, Loss: 3.72770, Epoch Loss: 0.46779\n",
      "Validation: Accuracy: 16.9%, Avg loss: 0.103660 \n",
      "\n",
      "Epoch 44:\n",
      "Training: Accuracy 8.3%, Loss: 3.71097, Epoch Loss: 0.46569\n",
      "Validation: Accuracy: 14.0%, Avg loss: 0.107470 \n",
      "\n",
      "Epoch 45:\n",
      "Training: Accuracy 8.1%, Loss: 3.67702, Epoch Loss: 0.46143\n",
      "Validation: Accuracy: 13.9%, Avg loss: 0.105178 \n",
      "\n",
      "Epoch 46:\n",
      "Training: Accuracy 7.0%, Loss: 3.80562, Epoch Loss: 0.47757\n",
      "Validation: Accuracy: 14.1%, Avg loss: 0.105656 \n",
      "\n",
      "Epoch 47:\n",
      "Training: Accuracy 7.7%, Loss: 3.75006, Epoch Loss: 0.47060\n",
      "Validation: Accuracy: 16.2%, Avg loss: 0.102676 \n",
      "\n",
      "Epoch 48:\n",
      "Training: Accuracy 9.1%, Loss: 3.65882, Epoch Loss: 0.45915\n",
      "Validation: Accuracy: 16.2%, Avg loss: 0.102363 \n",
      "\n",
      "Epoch 49:\n",
      "Training: Accuracy 9.9%, Loss: 3.68271, Epoch Loss: 0.46214\n",
      "Validation: Accuracy: 14.7%, Avg loss: 0.104283 \n",
      "\n",
      "Epoch 50:\n",
      "Training: Accuracy 9.8%, Loss: 3.67819, Epoch Loss: 0.46158\n",
      "Validation: Accuracy: 12.9%, Avg loss: 0.104272 \n",
      "\n",
      "Epoch 51:\n",
      "Training: Accuracy 9.5%, Loss: 3.67642, Epoch Loss: 0.46135\n",
      "Validation: Accuracy: 16.1%, Avg loss: 0.103976 \n",
      "\n",
      "Epoch 52:\n",
      "Training: Accuracy 8.9%, Loss: 3.62254, Epoch Loss: 0.45459\n",
      "Validation: Accuracy: 16.7%, Avg loss: 0.103139 \n",
      "\n",
      "Epoch 53:\n",
      "Training: Accuracy 9.1%, Loss: 3.64347, Epoch Loss: 0.45722\n",
      "Validation: Accuracy: 13.4%, Avg loss: 0.103436 \n",
      "\n",
      "Epoch 54:\n",
      "Training: Accuracy 8.1%, Loss: 3.69306, Epoch Loss: 0.46344\n",
      "Validation: Accuracy: 17.7%, Avg loss: 0.099769 \n",
      "\n",
      "Epoch 55:\n",
      "Training: Accuracy 9.6%, Loss: 3.57313, Epoch Loss: 0.44839\n",
      "Validation: Accuracy: 15.0%, Avg loss: 0.103889 \n",
      "\n",
      "Epoch 56:\n",
      "Training: Accuracy 9.4%, Loss: 3.51309, Epoch Loss: 0.44086\n",
      "Validation: Accuracy: 16.5%, Avg loss: 0.100917 \n",
      "\n",
      "Epoch 57:\n",
      "Training: Accuracy 10.5%, Loss: 3.58041, Epoch Loss: 0.44931\n",
      "Validation: Accuracy: 14.4%, Avg loss: 0.101423 \n",
      "\n",
      "Epoch 58:\n",
      "Training: Accuracy 11.2%, Loss: 3.59445, Epoch Loss: 0.45107\n",
      "Validation: Accuracy: 19.1%, Avg loss: 0.098926 \n",
      "\n",
      "Epoch 59:\n",
      "Training: Accuracy 10.1%, Loss: 3.61295, Epoch Loss: 0.45339\n",
      "Validation: Accuracy: 17.2%, Avg loss: 0.099476 \n",
      "\n",
      "Epoch 60:\n",
      "Training: Accuracy 11.1%, Loss: 3.52729, Epoch Loss: 0.44264\n",
      "Validation: Accuracy: 16.2%, Avg loss: 0.101699 \n",
      "\n",
      "Epoch 61:\n",
      "Training: Accuracy 9.2%, Loss: 3.60945, Epoch Loss: 0.45295\n",
      "Validation: Accuracy: 16.1%, Avg loss: 0.100885 \n",
      "\n",
      "Epoch 62:\n",
      "Training: Accuracy 10.7%, Loss: 3.53234, Epoch Loss: 0.44327\n",
      "Validation: Accuracy: 14.6%, Avg loss: 0.102902 \n",
      "\n",
      "Epoch 63:\n",
      "Training: Accuracy 10.8%, Loss: 3.52535, Epoch Loss: 0.44240\n",
      "Validation: Accuracy: 18.5%, Avg loss: 0.099846 \n",
      "\n",
      "Epoch 64:\n",
      "Training: Accuracy 11.8%, Loss: 3.55710, Epoch Loss: 0.44638\n",
      "Validation: Accuracy: 17.9%, Avg loss: 0.099703 \n",
      "\n",
      "Epoch 65:\n",
      "Training: Accuracy 9.6%, Loss: 3.55005, Epoch Loss: 0.44550\n",
      "Validation: Accuracy: 16.4%, Avg loss: 0.099589 \n",
      "\n",
      "Epoch 66:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingDataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossFunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     valLoss, valAccuracy \u001b[38;5;241m=\u001b[39m validateModel(validationDataloader, model, lossFunction)\n\u001b[1;32m     20\u001b[0m     valLosses\u001b[38;5;241m.\u001b[39mappend(valLoss)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mtrainModel\u001b[0;34m(dataloader, model, lossFunction, optimizer)\u001b[0m\n\u001b[1;32m     15\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossFunction(pred, y)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m currentLoss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "learningRate = 0.0001\n",
    "weightDecay = 0.0001\n",
    "epochs = 1000\n",
    "\n",
    "bestValAccuracy = 0.0\n",
    "bestValLoss = float('inf')\n",
    "bestEpoch = 0\n",
    "\n",
    "valLosses = []\n",
    "valAccuracies = []\n",
    "\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=450, gamma=0.99)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}:')\n",
    "    trainModel(trainingDataloader, model, lossFunction, optimizer)\n",
    "    valLoss, valAccuracy = validateModel(validationDataloader, model, lossFunction)\n",
    "    valLosses.append(valLoss)\n",
    "    valAccuracies.append(valAccuracy)\n",
    "    scheduler.step()\n",
    "\n",
    "    if (valLoss < bestValLoss):\n",
    "        bestValLoss = valLoss\n",
    "        bestValAccuracy = valAccuracy\n",
    "        bestEpoch = epoch+1\n",
    "\n",
    "testModel(testDataloader, model, lossFunction)\n",
    "print(f'Best Accuracy: {bestValAccuracy}. Best Loss: {bestValLoss} Best Epoch: {bestEpoch}')\n",
    "print(\"Finished\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(),'bestModel.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing: Accuracy: 75.2%, Avg loss: 0.032671 - 1000 epochs (434 mins - 7 hrs 12 mins)\n",
    "\n",
    "Testing: Accuracy: 73.9%, Avg loss: 0.034663 - 1000 epochs (416 mins - 6 hrs 55 mins)\n",
    "\n",
    "Testing: Accuracy: 72.8%, Avg loss: 0.036552 - 1000 epochs (426 mins - 7 hrs 6 mins)\n",
    "\n",
    "Testing: Accuracy: 66.2%, Avg loss: 0.041491 - 1000 epochs (619 mins - 10 hrs 20 mins)\n",
    "\n",
    "Testing: Accuracy 62.7% Avg loss: 0.053697 - 500 epochs (3hrs 30 mins)\n",
    "\n",
    "Testing: Accuracy: 66.4%, Avg loss: 0.044697 - 500 epochs (3hrs 30 mins)\n",
    "\n",
    "Testing: Accuracy: 59.3%, Avg loss: 0.060451 - 500 epochs (3 hrs 25 mins)- potentially done with 350 epochs. \n",
    "\n",
    "Testing: Accuracy: 48.1%, Avg loss: 0.069918  - 200 epochs\n",
    "\n",
    "Testing: Accuracy: 48.5%, Avg loss: 0.063186 - 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "512\n",
      "Testing: Accuracy: 73.9%, Avg loss: 0.034648 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load and Test the trained model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "testTransform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "    ])\n",
    "\n",
    "testData = datasets.Flowers102(\n",
    "    root = \"./datasets\",\n",
    "    split = \"test\",\n",
    "    transform = testTransform,\n",
    "    download = True)\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, classAmount):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.convStack =  nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "\n",
    "            nn.Conv2d(32, 32, 3, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),          \n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(128, 128, 3, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(256, 256, 3, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        dummyInput = torch.zeros(1, 3, 256, 256)\n",
    "        dummyOutput = self.convStack(dummyInput)\n",
    "        self.convOutputSize = dummyOutput.view(1, -1).size(1)\n",
    "        print(self.convOutputSize)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.convOutputSize, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, classAmount)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convStack(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "newModel = NeuralNetwork(102).to(device)\n",
    "\n",
    "newModel.load_state_dict(torch.load('Model.pt', map_location=torch.device('cpu')))\n",
    "newModel.eval()\n",
    "\n",
    "def testTrainedModel(dataloader, model, lossFunction):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    testLoss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            total += y.size(0)\n",
    "            testLoss += lossFunction(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    testLoss = testLoss/total\n",
    "    correct = (correct/total)  * 100\n",
    "    print(f\"Testing: Accuracy: {(correct):>0.1f}%, Avg loss: {testLoss:>8f} \\n\")\n",
    "\n",
    "testDataloader = DataLoader(testData, batch_size = 32, shuffle = False, num_workers = 6)\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "\n",
    "testTrainedModel(testDataloader, newModel, lossFunction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP5pQ4pqThi+Psu2q1m/Fu+",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
